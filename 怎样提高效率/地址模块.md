从上一章我们知道,获取链接是爬虫工作的第一步.

除去我们事先给与的链接外，大部分链接都要靠通过挖掘现有的链接得到，而网上的网页很多是重复的，过期的和没有参考性的。那么，我们如何在爬取过程中避免出现这些情况呢？我们可以通过地址去重来避免抓取的重复页面，然后通过价值算法来提升优质链接的优先级来避免抓取到无参考性的和过期的页面.

  
首先我们来说下地址去重。这指的是由于获取方式的不固定，那么就有可能存在地址重复的问题，我们可以在链接送到下载模块之前，对整个地址池内的链接进行去重操作.这样我们就可以防止同一个链接反复多次抓取从而浪费资源和性能。

  
然后我们通过通过对链接进行价值估算来实现优先级顺序.可以根据权重来决定传递给下载模块的链接.既然我们目标是最短的时间内获取到最优质的数据，那么肯定是高价值的页面具有最高优先级.我们可以通过拟定的权重（例如数据准确，更新迅速，网页接口好等等）,将地址分为几个档次，在传递链接时，优先提供价值高的链接。这样可以有效的提高获取优质数量的效率.

  
比如说，我们可以通过日后讲的消息队列来实现价值估算。最开始，我们给每个地址加上相同的时间属性.该时间属性是用来表示我们应该多久重复来爬取一次.然后经过几次测试后，将更新快的与不更新的区分开，最终打上优先级.然后在爬虫取出链接时，可以根据优先级来获取链接.简化版流程图如下:![](/assets/QQ截图20170805003616.png)

这流程图只画了了一个流程，我们可以通过多次流程来更精准的确定各链接的优先级.



最后的地址模块内部的流程图就是这样：![](/assets/QQ截图20170805003332.png)

