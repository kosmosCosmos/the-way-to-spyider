从本章开始，我们来讲讲爬虫的优化.不知道你有没有遇到这种情况，那就是别人的一天能上百万数量级还都是优质数据，而同等配置的你的爬虫一天只有十几万而且质量还是参差不齐呢？那就是你没有还好的优化爬虫.在追求爬虫的极致性能的今天，如何优化爬虫都将是爬虫爱好者们面对的重要问题。那么，从现在开始，我们将逐步的从起始出发，一步步的为爬虫更好的爬取信息而努力.

首先，先来聊聊爬虫是如何从一条命令进化到如今的大规模商用爬虫的.因为只有了解了爬虫的构造，我们才能得心应手，看病下药，找到爬虫的痛点，一一解决.首先，最简单的爬虫是wget.不要怀疑，wget也是爬虫，不过是没有解析模块和地址调度模块的爬虫罢了.但这样有个问题，这样无用数据太多而且太占存储空间.所以我们得解析页面,通过HTML DOM等各种解析工具从而在读取到网页的同时只取出我们所需的数据，而其他的就丢弃.这样就极大的减少了存储空间.所以，爬虫本身的基本架构就类似与这发展史，分成这么几个部分：地址模块.下载与解析模块.存储模块以及核心中枢模块.

地址模块主要负责提供需要爬取的目标地址，不管该地址是爬取过程中发散得到的还是原有的种子链接.通过各种算法决定下一个提供给下载模块的地址是什么，起到地址调度分配的功能.这方面我们可以通过url去重以及算法优化来提升性能和可用数据的占比.

下载解析模块通过发送GET/POST HTTP请求给目标服务器来获得回应，然后通过上述的各种方式来获取我们想得到的数据。这方面我们可以通过建立DOM树使用HTML DOM来灵活的选取HTML中的各个元素.

存储模块主要负责将获取到的数据通过各种方式存储起来.一般来说，存储是爬虫整体架构中消耗性能最多的。所以，我们在这上面要下的功夫也就最多.我们的对策就是通过ORM实现了批量存储，减轻了存储压力然后通过引入中间件，使得解析与存储模块分离极大的加速了爬虫的运行效率.

核心中枢模块是负责爬虫整体的运转和各模块之间的调度.属于控制模块，不参与实际数据的处理.

大概运行逻辑是这样的：![](/assets/QQ截图20170805001017.png)

从流程图中我们也可以看到，现在各模块都缠绕在一起，这样就会造成很大隐患.例如爬虫部分模块瘫痪后会蔓延至整体模块..所以，我们要引入中间件，不仅仅是为了分离模块来达到加速的问题，同时通过将各模块之间拆散用中间件相连，可以有效的防止整体程序都出错的可能.而加入中间件之后的流程图见应对性能瓶颈的消息队列章.

爬虫的构造我们已经明了了。那么你心在心里有了什么优化的想法呢？没有也不要紧，接下来我们会逐步的根据爬虫的不同模块进行针对性的优化.敬请期待.

